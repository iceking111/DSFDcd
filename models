import torch
import torch.nn as nn
import torch.nn.functional as F
from DSFDcd.models.pre import StemLayer, resblock
from torch.distributions import Independent, Normal
import numpy as np
from typing import Tuple, List, Union, Callable, Optional
class _2DConvolutionalBlock(nn.Module):
    def __init__(self, in_ch: int, out_ch: int, mid_ch: int = None):
        super(_2DConvolutionalBlock, self).__init__()
        self._convolutional_path = nn.Sequential(
            nn.Conv2d(in_ch, mid_ch if mid_ch is not None else out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(mid_ch if mid_ch is not None else out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(mid_ch if mid_ch is not None else out_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    def _feature_transformation(self, x: torch.Tensor) -> torch.Tensor:
        return self._convolutional_path(x)
class _DownsamplingModule(nn.Module):
    def __init__(self, in_ch: int, out_ch: int):
        super(_DownsamplingModule, self).__init__()
        self._feature_transform = nn.Sequential(
            nn.MaxPool2d(2),
            _2DConvolutionalBlock(in_ch, out_ch)
        )
    def _downsample_operation(self, x: torch.Tensor) -> torch.Tensor:
        return self._feature_transform(x)
class _UpsamplingModule(nn.Module):
    def __init__(self, in_ch: int, out_ch: int, bilinear: bool = True):
        super(_UpsamplingModule, self).__init__()
        self._upsample_op = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) if bilinear else \
            nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)
        self._conv_block = _2DConvolutionalBlock(in_ch, out_ch, in_ch // 2) if bilinear else \
            _2DConvolutionalBlock(in_ch, out_ch)

    def _upsample_operation(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
        x1 = self._upsample_op(x1)
        diffY, diffX = x2.size()[2] - x1.size()[2], x2.size()[3] - x1.size()[3]
        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])
        x = torch.cat([x2, x1], dim=1)
        return self._conv_block._feature_transformation(x)


class _OutputConvolution(nn.Module):
    def __init__(self, in_ch: int, out_ch: int):
        super(_OutputConvolution, self).__init__()
        self._conv_layer = nn.Conv2d(in_ch, out_ch, kernel_size=1)

    def _output_transformation(self, x: torch.Tensor) -> torch.Tensor:
        return self._conv_layer(x)


class _UNetBase(nn.Module):
    def __init__(self, n_ch: int, n_cls: int, bilinear: bool = True):
        super(_UNetBase, self).__init__()
        self._initial_transform = _2DConvolutionalBlock(n_ch, 32)
        self._down_transform1 = _DownsamplingModule(32, 64)
        self._down_transform2 = _DownsamplingModule(64, 128)
        self._down_transform3 = _DownsamplingModule(128, 256)
        factor = 2 if bilinear else 1
        self._down_transform4 = _DownsamplingModule(256, 512 // factor)
        self._up_transform1 = _UpsamplingModule(512, 256 // factor, bilinear)
        self._up_transform2 = _UpsamplingModule(256, 128 // factor, bilinear)
        self._up_transform3 = _UpsamplingModule(128, 64 // factor, bilinear)
        self._up_transform4 = _UpsamplingModule(64, 32, bilinear)
        self._output5 = _OutputConvolution(512, 256)
        self._output4 = _OutputConvolution(512, 256)
        self._output3 = _OutputConvolution(256, 128)
        self._output2 = _OutputConvolution(128, 64)
        self._output1 = _OutputConvolution(64, 32)


class UNet1(_UNetBase):
    def __init__(self, n_channels: int, n_classes: int, bilinear: bool = True):
        super(UNet1, self).__init__(n_channels, n_classes, bilinear)

    def _dual_input_processing(self, x: torch.Tensor, y: torch.Tensor) -> Tuple[
        torch.Tensor, torch.Tensor, torch.Tensor]:
        x1 = self._initial_transform._feature_transformation(x)
        x2 = self._down_transform1._downsample_operation(x1)
        x3 = self._down_transform2._downsample_operation(x2)
        x4 = self._down_transform3._downsample_operation(x3)
        x5 = self._down_transform4._downsample_operation(x4)

        y1 = self._initial_transform._feature_transformation(y)
        y2 = self._down_transform1._downsample_operation(y1)
        y3 = self._down_transform2._downsample_operation(y2)
        y4 = self._down_transform3._downsample_operation(y3)
        y5 = self._down_transform4._downsample_operation(y4)

        z5, z4 = x5 - y5, x4 - y4
        z3, z2 = x3 - y3, x2 - y2
        z1 = x1 - y1

        x = self._up_transform1._upsample_operation(z5, z4)
        x = self._up_transform2._upsample_operation(x, z3)
        x = self._up_transform3._upsample_operation(x, z2)
        x = self._up_transform4._upsample_operation(x, z1)
        return x, x5, y5


class UNet2(_UNetBase):
    def __init__(self, n_channels: int, n_classes: int, bilinear: bool = True):
        super(UNet2, self).__init__(n_channels, n_classes, bilinear)

    def _single_input_processing(self, x: torch.Tensor) -> torch.Tensor:
        x1 = self._initial_transform._feature_transformation(x)
        x2 = self._down_transform1._downsample_operation(x1)
        x3 = self._down_transform2._downsample_operation(x2)
        x4 = self._down_transform3._downsample_operation(x3)
        x5 = self._down_transform4._downsample_operation(x4)

        x = self._up_transform1._upsample_operation(x5, x4)
        x = self._up_transform2._upsample_operation(x, x3)
        x = self._up_transform3._upsample_operation(x, x2)
        x = self._up_transform4._upsample_operation(x, x1)
        return x


class _SelfFeatureDecoder(nn.Module):
    def __init__(self, start_ch: int):
        super(_SelfFeatureDecoder, self).__init__()
        self._conv1 = nn.Conv2d(start_ch, start_ch * 2, kernel_size=3, stride=1, padding=1)
        self._bn1 = nn.BatchNorm2d(start_ch * 2)
        self._resblock1 = resblock(start_ch // 2, start_ch // 2)
        self._conv2 = nn.Conv2d(start_ch * 2, start_ch * 2, kernel_size=3, stride=1, padding=1)
        self._bn2 = nn.BatchNorm2d(start_ch * 2)
        self._resblock2 = resblock(start_ch // 4, start_ch // 4)
        self._conv3 = nn.Conv2d(start_ch * 2, start_ch, kernel_size=1, stride=1)
        self._bn3 = nn.BatchNorm2d(start_ch)
        self._activation = nn.ReLU()

    def _self_feature_extraction(self, x: torch.Tensor) -> torch.Tensor:
        x = self._conv1(x)
        x = self._bn1(x)
        x = self._activation(x)
        x = self._conv2(x)
        x = self._bn2(x)
        x = self._activation(x)
        x = self._conv3(x)
        x = self._bn3(x)
        return x


def _truncated_normal(tensor: torch.Tensor, mean: float = 0, std: float = 1) -> None:
    size = tensor.shape
    tmp = tensor.new_empty(size + (4,)).normal_()
    valid = (tmp < 2) & (tmp > -2)
    ind = valid.max(-1, keepdim=True)[1]
    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))
    tensor.data.mul_(std).add_(mean)


def _weight_initialization(m: nn.Module) -> None:
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
        _truncated_normal(m.bias, mean=0, std=0.001)


class _KLEncoder(nn.Module):
    def __init__(self, input_channels: int, num_filters: List[int], no_convs_per_block: int,
                 initializers: dict, padding: bool = True, posterior: bool = False):
        super(_KLEncoder, self).__init__()
        self._input_channels = input_channels + 4 if posterior else input_channels
        layers = []
        for i in range(len(num_filters)):
            input_dim = self._input_channels if i == 0 else num_filters[i - 1]
            output_dim = num_filters[i]
            if i != 0:
                layers.append(nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True))
            layers.extend([
                nn.Conv2d(input_dim, output_dim, kernel_size=3, padding=int(padding)),
                nn.ReLU(inplace=True)
            ])
            for _ in range(no_convs_per_block - 1):
                layers.extend([
                    nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=int(padding)),
                    nn.ReLU(inplace=True)
                ])
        self._feature_transform = nn.Sequential(*layers)
        self._feature_transform.apply(_weight_initialization)

    def _encoding_operation(self, input: torch.Tensor) -> torch.Tensor:
        return self._feature_transform(input)


class _AxisAlignedGaussian(nn.Module):
    def __init__(self, input_channels: int, num_filters: List[int], no_convs_per_block: int,
                 latent_dim: int, initializers: dict, posterior: bool = False):
        super(_AxisAlignedGaussian, self).__init__()
        self._latent_dim = latent_dim
        self._encoder = _KLEncoder(input_channels, num_filters, no_convs_per_block,
                                   initializers, posterior=posterior)
        self._projection_layer = nn.Conv2d(num_filters[-1], 2 * latent_dim, kernel_size=1, stride=1)
        _weight_initialization(self._projection_layer)

    def _input_preprocessing(self, input: torch.Tensor, segm: Optional[torch.Tensor] = None) -> torch.Tensor:
        if segm is not None:
            input = torch.cat((input, segm), dim=1)
        encoding = self._encoder._encoding_operation(input)
        encoding = torch.mean(encoding, dim=2, keepdim=True)
        encoding = torch.mean(encoding, dim=3, keepdim=True)
        return encoding

    def _distribution_generation(self, encoding: torch.Tensor) -> Independent:
        mu_log_sigma = self._projection_layer(encoding)
        mu_log_sigma = torch.squeeze(mu_log_sigma, dim=2)
        mu_log_sigma = torch.squeeze(mu_log_sigma, dim=2)
        mu, log_sigma = mu_log_sigma[:, :self._latent_dim], mu_log_sigma[:, self._latent_dim:]
        return Independent(Normal(loc=mu, scale=torch.exp(log_sigma)), 1)




class _ImageReconstruction(nn.Module):
    def __init__(self):
        super(_ImageReconstruction, self).__init__()
        self._conv1 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)
        self._bn1 = nn.BatchNorm2d(32)
        self._resblock1 = resblock(32, 32)
        self._conv2 = nn.Conv2d(32, 8, kernel_size=3, stride=1, padding=1)
        self._bn2 = nn.BatchNorm2d(8)
        self._resblock2 = resblock(8, 8)
        self._conv6 = nn.Conv2d(8, 3, kernel_size=3, stride=1, padding=1)
        self._bn6 = nn.BatchNorm2d(3)
        self._resblock6 = resblock(3, 3)
        self._conv7 = nn.Conv2d(3, 3, kernel_size=1, stride=1)
        self._tanh = nn.Tanh()
        self._relu = nn.ReLU()

    def _image_generation(self, feature1: torch.Tensor, feature2: torch.Tensor) -> torch.Tensor:
        out = torch.cat((feature1, feature2), dim=1)
        out = self._conv1(out)
        out = self._bn1(out)
        out = self._relu(out)
        out = self._resblock1(out)
        out = self._conv2(out)
        out = self._bn2(out)
        out = self._relu(out)
        out = self._resblock2(out)
        out = self._conv6(out)
        out = self._bn6(out)
        out = self._relu(out)
        out = self._resblock6(out)
        out = self._conv7(out)
        out = self._tanh(out)
        return out


class _SelfChangeDetector(nn.Module):
    def __init__(self):
        super(_SelfChangeDetector, self).__init__()
        self._conv1 = nn.Conv2d(32, 8, kernel_size=3, stride=1, padding=1)
        self._bn1 = nn.BatchNorm2d(8)
        self._resblock1 = resblock(8, 8)
        self._conv2 = nn.Conv2d(8, 4, kernel_size=3, stride=1, padding=1)
        self._bn2 = nn.BatchNorm2d(4)
        self._resblock2 = resblock(4, 4)
        self._conv5 = nn.Conv2d(4, 1, kernel_size=1, stride=1)
        self._relu = nn.ReLU()

    def _change_detection(self, feature1: torch.Tensor, feature2: torch.Tensor) -> torch.Tensor:
        out = torch.cat((feature1, feature2), dim=1)
        out = self._conv1(out)
        out = self._bn1(out)
        out = self._relu(out)
        out = self._resblock1(out)
        out = self._conv2(out)
        out = self._bn2(out)
        out = self._relu(out)
        out = self._resblock2(out)
        out = self._conv5(out)
        return out


class _TotalChangeDetector(nn.Module):
    def __init__(self):
        super(_TotalChangeDetector, self).__init__()
        self._conv1 = nn.Conv2d(96, 48, kernel_size=3, stride=1, padding=1)
        self._bn1 = nn.BatchNorm2d(48)
        self._resblock1 = resblock(48, 48)
        self._conv2 = nn.Conv2d(48, 12, kernel_size=3, stride=1, padding=1)
        self._bn2 = nn.BatchNorm2d(12)
        self._resblock2 = resblock(12, 12)
        self._conv3 = nn.Conv2d(12, 3, kernel_size=3, stride=1, padding=1)
        self._bn3 = nn.BatchNorm2d(3)
        self._resblock3 = resblock(3, 3)
        self._conv7 = nn.Conv2d(3, 1, kernel_size=1, stride=1)
        self._relu = nn.ReLU()

    def _total_change_estimation(self, feature1: torch.Tensor, feature2: torch.Tensor) -> torch.Tensor:
        out = torch.cat((feature1, feature2), dim=1)
        out = self._conv1(out)
        out = self._bn1(out)
        out = self._relu(out)
        out = self._resblock1(out)
        out = self._conv2(out)
        out = self._bn2(out)
        out = self._relu(out)
        out = self._resblock2(out)
        out = self._conv3(out)
        out = self._bn3(out)
        out = self._relu(out)
        out = self._resblock3(out)
        out = self._conv7(out)
        return out


class _FeatureCombiner(nn.Module):
    def __init__(self, inchannels: int, num_filters: List[int], latent_dim: int,
                 num_output_channels: int, no_convs: int, initializers: dict, use_tile: bool = True):
        super(_FeatureCombiner, self).__init__()
        self._use_tile = use_tile
        self._device = 'cuda:0'
        if use_tile:
            layers = [nn.Conv2d(38, num_filters[0], kernel_size=1), nn.ReLU(inplace=True)]
            for _ in range(no_convs - 2):
                layers.extend([
                    nn.Conv2d(num_filters[0], num_filters[_], kernel_size=1),
                    nn.ReLU(inplace=True)
                ])
            self._feature_transform = nn.Sequential(*layers)
            self._output_layer = nn.Conv2d(num_filters[0], num_output_channels, kernel_size=1)
            self._feature_transform.apply(_weight_initialization)
            self._output_layer.apply(_weight_initialization)
        self._softmax = torch.nn.Softmax(dim=1)

    def _tile_operation(self, a: torch.Tensor, dim: int, n_tile: int) -> torch.Tensor:
        init_dim = a.size(dim)
        repeat_idx = [1] * a.dim()
        repeat_idx[dim] = n_tile
        a = a.repeat(*repeat_idx)
        order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(
            self._device)
        return torch.index_select(a, dim, order_index)

    def _feature_combination(self, feature_map: torch.Tensor, z: torch.Tensor,
                             use_softmax: bool = False) -> torch.Tensor:
        if self._use_tile:
            z = torch.unsqueeze(z, 2)
            z = self._tile_operation(z, 2, feature_map.shape[2])
            z = torch.unsqueeze(z, 3)
            z = self._tile_operation(z, 3, feature_map.shape[3])
            feature_map = torch.cat((feature_map, z), dim=1)
            output = self._feature_transform(feature_map)
            output = self._output_layer(output)
            if use_softmax:
                output = self._softmax(output)
        return output


class _ConvFeatureBlock(nn.Module):
    def __init__(self, input_channels: int, num_filters: List[int], padding: bool = True):
        super(_ConvFeatureBlock, self).__init__()
        layers = []
        for i in range(len(num_filters)):
            input_dim = input_channels if i == 0 else num_filters[i - 1]
            output_dim = num_filters[i]
            if i != 0:
                layers.append(nn.AvgPool2d(kernel_size=2, stride=2, padding=0))
            layers.extend([
                nn.Conv2d(input_dim, output_dim, kernel_size=3, padding=int(padding)),
                nn.ReLU(inplace=True),
                nn.Dropout2d(0.1),
                nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=int(padding)),
                nn.ReLU(inplace=True)
            ])
        self._feature_transform = nn.Sequential(*layers)
        self._feature_transform.apply(_weight_initialization)

    def _feature_extraction(self, input: torch.Tensor) -> torch.Tensor:
        return self._feature_transform(input)


class _LatentProjection(nn.Module):
    def __init__(self, latent_dim: int):
        super(_LatentProjection, self).__init__()
        self._expert_heads_self = nn.ModuleList([_ConvFeatureBlock(16, [32, 16, 6], 3)])
        self._expert_heads_total = nn.ModuleList([_ConvFeatureBlock(48, [32, 16, 6], 3)])
        self._pooling = nn.AdaptiveAvgPool2d([1, 1])
        self._softmax = torch.nn.Softmax(dim=2)

    def _latent_similarity(self, feature_map: torch.Tensor, z_set: torch.Tensor) -> torch.Tensor:
        feats = self._expert_heads_self[0]._feature_extraction(feature_map) if feature_map.shape[1] == 16 else \
            self._expert_heads_total[0]._feature_extraction(feature_map)
        bs = feature_map.shape[0]
        global_z = self._pooling(feats).view(bs, self._latent_dim, -1).permute(0, 2, 1)
        similarity = torch.bmm(global_z, z_set.permute(0, 2, 1))
        similarity = self._softmax(similarity)
        output = torch.bmm(similarity, z_set)
        return output


class Net(nn.Module):
    def __init__(self, num_filters: List[int] = [5, 32, 64], latent_dim: int = 2,
                 no_convs_fcomb: int = 2, reg_factor: float = 1.0, original_backbone: bool = False):
        super(Net, self).__init__()
        self._device = 'cuda:0'
        self.to(self._device)
        self._stem_transform = StemLayer(in_channels=3, out_channels=16)
        self._unet1 = UNet1(3, 32)
        self._unet2 = UNet2(3, 32)
        self._self_feature_decoder = _SelfFeatureDecoder(start_channel=32)
        self._change_prior = AxisAlignedConvGaussian(input_channels=3, num_filters=[16, 32, 64],
                                                     no_convs_per_block=3, latent_dim=6,
                                                     initializers={'w': 'he_normal', 'b': 'normal'})
        self._nonchange_prior = AxisAlignedConvGaussian(input_channels=3, num_filters=[16, 32, 64],
                                                        no_convs_per_block=3, latent_dim=6,
                                                        initializers={'w': 'he_normal', 'b': 'normal'})
        self._change_post = AxisAlignedConvGaussian_Nonchange(input_channels=32,
                                                     num_filters=[16, 32, 64],
                                                     no_convs_per_block=3, latent_dim=6,
                                                     initializers={'w': 'he_normal', 'b': 'normal'})
        self._nonchange_post = AxisAlignedConvGaussian__Nonchange(input_channels=32,
                                                        num_filters=[16, 32, 64],
                                                        no_convs_per_block=3, latent_dim=6,
                                                        initializers={'w': 'he_normal', 'b': 'normal'})
        self._to_image = _ImageReconstruction()
        self._get_self = _SelfChangeDetector()
        self._get_total = _TotalChangeDetector()
        self._fcomb_prototype = _FeatureCombiner(16, num_filters, latent_dim, 1,
                                                 no_convs_fcomb, {'w': 'orthogonal', 'b': 'normal'}, use_tile=True)
        self._fcomb_result = _FeatureCombiner(64, num_filters, latent_dim, 1,
                                              no_convs_fcomb, {'w': 'orthogonal', 'b': 'normal'}, use_tile=True)
        self._proj_heads = _LatentProjection(latent_dim).to(self._device)
        self._conv_layer0 = nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1)
        self._conv_layer1 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)
        self._conv_layer2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)
        self._conv_layer3 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)
        self._conv_layer4 = nn.Conv2d(48, 16, kernel_size=3, stride=1, padding=1)
        self._conv_layer5 = nn.Conv2d(96, 32, kernel_size=3, stride=1, padding=1)
        self._conv_layer6 = nn.Conv2d(192, 64, kernel_size=3, stride=1, padding=1)
        self._conv_layer7 = nn.Conv2d(384, 128, kernel_size=3, stride=1, padding=1)
        self._conv_list1 = nn.Conv2d(100, 50, kernel_size=3, stride=1, padding=1)
        self._bn_list1 = nn.BatchNorm2d(50)
        self._conv_list2 = nn.Conv2d(50, 25, kernel_size=3, stride=1, padding=1)
        self._bn_list2 = nn.BatchNorm2d(25)
        self._conv_list3 = nn.Conv2d(25, 1, kernel_size=3, stride=1, padding=1)
        self._relu = nn.ReLU()

    def _latent_sampling(self, dist: Independent, sample_num: int = 20, training: bool = True) -> List[torch.Tensor]:
        samples = []
        for _ in range(sample_num):
            z = dist.rsample() if training else dist.sample()
            samples.append(z.unsqueeze(1))
        return samples

    def _feature_and_sample_combination(self, x: torch.Tensor, x_set: List[torch.Tensor]) -> torch.Tensor:
        num_z = len(x_set)
        cases = []
        for idx in range(num_z):
            z_case = x_set[idx]
            if x.shape[1] == 16:
                case = self._fcomb_prototype._feature_combination(x, z_case, use_softmax=False)
            else:
                case = self._fcomb_result._feature_combination(x, z_case, use_softmax=False)
            cases.append(case)
        return torch.cat(cases, dim=1)

    def _kl_divergence_calculation(self, p: Independent, q: Independent) -> torch.Tensor:
        return torch.mean(torch.distributions.kl.kl_divergence(p, q), dim=0)

    def _training_mode_operation(self, input1: torch.Tensor, input2: torch.Tensor, label: torch.Tensor, epochid: int) -> \
    Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        x_total, x2, y2 = self._unet1._dual_input_processing(input1, input2)
        self._x_nonchange = self._self_feature_decoder._self_feature_extraction(x_total)
        self._x_change = x_total - self._x_nonchange
        input_diff = input1 - input2
        x_total_post = self._unet2._single_input_processing(input_diff)

        self._input_change_prior = self._change_prior._forward_operation(input_diff)
        self._input_nonchange_prior = self._nonchange_prior._forward_operation(input_diff)

        change_post = x_total_post * label
        nonchange_post = x_total_post * (1 - label)

        prototypes_change = self._latent_sampling(self._input_change_prior, 20, True)
        prototypes_change_feature = self._feature_and_sample_combination(self._x_change, prototypes_change)

        prototypes_nonchange = self._latent_sampling(self._input_nonchange_prior, 20, True)
        prototypes_nonchange_feature = self._feature_and_sample_combination(self._x_nonchange, prototypes_nonchange)

        self._label_change_post = self._change_post._forward_operation(change_post)
        self._label_nonchange_post = self._nonchange_post._forward_operation(nonchange_post)

        result_change = self._latent_sampling(self._label_change_post, 1, True)
        result_change_feature = self._feature_and_sample_combination(self._x_change, result_change)[0]

        result_nonchange = self._latent_sampling(self._label_nonchange_post, 1, True)
        result_nonchange_feature = self._feature_and_sample_combination(self._x_nonchange, result_nonchange)[0]

        map_output = torch.cat((result_change_feature, result_nonchange_feature), dim=1)
        map2_output = torch.cat((prototypes_change[0], prototypes_nonchange[0]), dim=1)

        kl_loss = self._kl_divergence_calculation(self._input_change_prior, self._label_change_post) + \
                  self._kl_divergence_calculation(self._input_nonchange_prior, self._label_nonchange_post)

        return kl_loss, prototypes_change_feature, prototypes_nonchange_feature, map_output, map2_output

    def _inference_mode_operation(self, input1: torch.Tensor, input2: torch.Tensor) -> torch.Tensor:
        x_total, _, _ = self._unet1._dual_input_processing(input1, input2)
        self._x_nonchange = self._self_feature_decoder._self_feature_extraction(x_total)
        self._x_change = x_total - self._x_nonchange
        input_diff = input1 - input2

        prototypes_change = self._latent_sampling(self._change_prior._forward_operation(input_diff), 1, False)
        prototypes_change_feature = self._fcomb_prototype._feature_combination(self._x_change, prototypes_change[0],
                                                                               use_softmax=False)

        prototypes_nonchange = self._latent_sampling(self._nonchange_prior._forward_operation(input_diff), 1, False)
        prototypes_nonchange_feature = self._fcomb_prototype._feature_combination(self._x_nonchange,
                                                                                  prototypes_nonchange[0],
                                                                                  use_softmax=False)

        return torch.cat((prototypes_change_feature, prototypes_nonchange_feature), dim=1)

    def model_pipeline(self, input1: torch.Tensor, input2: torch.Tensor, label: Optional[torch.Tensor] = None,
                       training: bool = False, epochid: int = 1) -> Union[
        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:
        if training:
            return self._training_mode_operation(input1, input2, label, epochid)
        else:
            return self._inference_mode_operation(input1, input2)

